# FAQ

**Q: How can I join VBench Leaderboard?**<br>
A: There are 3 options to join the leaderboard:<br>
Option | Sampling Party | Evaluation Party |              Comments                         |
| :---: | :---: |  :---: |        :--------------    | 
| 1Ô∏è‚É£ | VBench Team | VBench Team | VBench Team handles everything ‚Äî ideal for both open-source and closed-source models (if API access is provided). We periodically allocate resources to sample newly released models and perform evaluations. You can request us to perform sampling and evaluation, but the progress depends on our available resources. |
| 2Ô∏è‚É£ | Your Team | VBench Team | Submit your video samples via this [Google Form](https://forms.gle/Dy26sRobB6vouQZC7).If you prefer to evaluate on your own, refer to Option 3. |
| 3Ô∏è‚É£ | Your Team | Your Team | Already performed the full VBench evaluation? Submit your `eval_results.zip` files to the [VBench Leaderboard](https://huggingface.co/spaces/Vchitect/VBench_Leaderboard)'s `[T2V]Submit here!` form. The evaluation results will be automatically updated to the leaderboard. Make sure to also fill in model details in the `[T2V]Submit here!` form. Submissions lacking essential information may be removed periodically. |

üîç Transparency Note: Each leaderboard entry clearly states who performed the sampling and evaluation.



**Q: What if my model has been updated and now performs better? Can I make a new submission to the VBench Leaderboard?**<br>
A: Yes, you can submit an updated version of your model. Please clearly specify what changes have been made and how this version differs from your previous submission. Updates will only be evaluated if the modifications are explicitly described.

**Q: After sharing a cloud storage link to our sampled videos, how long will it take to receive the evaluation results?**<br>
Once we receive both your cloud storage link and the corresponding model information, evaluation typically takes 1‚Äì2 weeks, depending on the current queue.

**Q: Does the evaluation process use the same method as in the code repository, or is there any manual intervention?**<br>
The evaluation strictly follows the open-source procedure provided in the official code repository. There is no manual intervention or human judgment involved.

**Q: If the evaluation results are unsatisfactory, can we resubmit new videos for the same model?**<br>
Evaluation results are final and tied to the specific model version submitted. However, you may submit a newer checkpoint or updated version of the model for a fresh evaluation. In such cases, please include a brief explanation of the updates. Previous results will remain on the leaderboard and will not be removed.