# Human Anomaly Dataset and Training Framework

This part provides an open-source dataset and training framework for Human Anomaly Detection.
---

## Data Structure

The dataset and related resources are organized as follows:

```
VBench-2.0_human_anomaly_root/
├── dataset/                # Main directory for the image dataset
│   ├── all_images.zip      # Compressed file containing all images
│   ├── face_train.jsonl    # Face training set annotations
│   ├── face_test.jsonl     # Face testing set annotations
│   ├── hand_train.jsonl    # Hand training set annotations
│   ├── hand_test.jsonl     # Hand testing set annotations
│   ├── human_train.jsonl   # Human training set annotations
│   ├── human_test.jsonl    # Human testing set annotations
├── src_video/              # Raw video data used to construct the dataset
│   ├── CogVideo/           # Videos generated by CogVideo1.5
│   ├── CogVideo-1.0/       # Videos generated by CogVideo1.0
│   ├── vad_real/           # Real-world videos
```

Each JSONL file contains annotations for the corresponding images. Each line in the JSONL file represents a single sample in the following format:
```json
["image_filename.jpg", label, score]
```

- `image_filename.jpg`: The base name of the image file.
- `label`: The ground truth label (e.g., class or pose information).
- `score`: Confidence score (unused).

---

## How to Train

### Prerequisites
- Follow VBench2.0 environment set-up

### Steps to Train
1. **Download Dataset**:
   - Download the dataset files from the following URL:
     ```bash
     git clone https://huggingface.co/datasets/Vchitect/VBench-2.0_human_anomaly
     ```

1. **Extract Images**:
   - Unzip the `all_images.zip` file into the `dataset/` directory:
     ```bash
     unzip dataset/all_images.zip -d dataset/
     ```

2. **Run Training**:
   - Execute the training script:
     ```bash
     ./run_train.sh
     ```
   - This script will automatically load the training and testing datasets, configure the model, and begin training.


### Notes
- Ensure that all paths in the JSONL files correctly point to the extracted images.
- Modify the configuration file (`config.yaml` or similar) if you wish to customize training parameters.

---

## License

This project is released under the [MIT License](LICENSE). You are free to use, modify, and distribute the dataset and code for academic and commercial purposes, provided you include proper attribution.

---


